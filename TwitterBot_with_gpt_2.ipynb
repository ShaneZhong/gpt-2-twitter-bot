{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TwitterBot with gpt-2 Template.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI-HVDbQS9dF",
        "colab_type": "text"
      },
      "source": [
        "# Twitter Bot with GPT-2\n",
        "\n",
        "  [Twitter Account link](https://twitter.com/ai_telling) <br>\n",
        "[Twitter Developer link](https://developer.twitter.com/en/apps/16374336)\n",
        "\n",
        "## Background\n",
        "In this Jupyter notebook you can play around with the small and medium version (117M/334M) of **Open AI's GPT-2** Model from the paper *[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf).*\n",
        "\n",
        "According to the authors, the GPT-2 algorithm was trained on the task of *language modeling*--- which tests a program's ability to predict the next word in a given sentence--by ingesting huge numbers of articles, blogs, and websites. By using just this data it achieved state-of-the-art scores on a number of unseen language tests, an achievement known as *zero-shot learning.* It can also perform other writing-related tasks, like translating text from one language to another, summarizing long articles, and answering trivia questions.\n",
        "\n",
        "Open AI decided not to release the dataset, training code, or the full GPT-2 model weights. This is due to the concerns about large language models being used to generate deceptive, biased, or abusive language at scale. Some examples of the applications of these models for malicious purposes are:\n",
        "* Generate misleading news articles\n",
        "* Impersonate others online\n",
        "* Automate the production of abusive or faked content to post on social media\n",
        "* Automate the production of spam/phishing content\n",
        "\n",
        "As one can imagine, this combined with recent advances in generation of synthetic imagery, audio, and video implies that it's never been easier to create fake content and spread disinformation at scale. The public at large will need to become more skeptical of the content they consume online. \n",
        "\n",
        "----\n",
        "\n",
        "**PRs are welcomed !**\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "\n",
        "## Steps\n",
        "Before starting, is recommended to set *Runtime Type* to *GPU* on the top menu bar.\n",
        "\n",
        "\n",
        "###1. Installation\n",
        "Download the model data and istall Python libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKqlSCrpS9dH",
        "colab_type": "code",
        "outputId": "4523bdf5-4077-4a62-cb49-5e8a518a9124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "!git clone https://github.com/ShaneZhong/gpt-2-twitter-bot/\n",
        "import os\n",
        "os.chdir('gpt-2-twitter-bot')\n",
        "!python download_model.py 117M\n",
        "!python download_model.py 345M\n",
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2-twitter-bot'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/27)   \u001b[K\rremote: Counting objects:   7% (2/27)   \u001b[K\rremote: Counting objects:  11% (3/27)   \u001b[K\rremote: Counting objects:  14% (4/27)   \u001b[K\rremote: Counting objects:  18% (5/27)   \u001b[K\rremote: Counting objects:  22% (6/27)   \u001b[K\rremote: Counting objects:  25% (7/27)   \u001b[K\rremote: Counting objects:  29% (8/27)   \u001b[K\rremote: Counting objects:  33% (9/27)   \u001b[K\rremote: Counting objects:  37% (10/27)   \u001b[K\rremote: Counting objects:  40% (11/27)   \u001b[K\rremote: Counting objects:  44% (12/27)   \u001b[K\rremote: Counting objects:  48% (13/27)   \u001b[K\rremote: Counting objects:  51% (14/27)   \u001b[K\rremote: Counting objects:  55% (15/27)   \u001b[K\rremote: Counting objects:  59% (16/27)   \u001b[K\rremote: Counting objects:  62% (17/27)   \u001b[K\rremote: Counting objects:  66% (18/27)   \u001b[K\rremote: Counting objects:  70% (19/27)   \u001b[K\rremote: Counting objects:  74% (20/27)   \u001b[K\rremote: Counting objects:  77% (21/27)   \u001b[K\rremote: Counting objects:  81% (22/27)   \u001b[K\rremote: Counting objects:  85% (23/27)   \u001b[K\rremote: Counting objects:  88% (24/27)   \u001b[K\rremote: Counting objects:  92% (25/27)   \u001b[K\rremote: Counting objects:  96% (26/27)   \u001b[K\rremote: Counting objects: 100% (27/27)   \u001b[K\rremote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects:   4% (1/21)   \u001b[K\rremote: Compressing objects:   9% (2/21)   \u001b[K\rremote: Compressing objects:  14% (3/21)   \u001b[K\rremote: Compressing objects:  19% (4/21)   \u001b[K\rremote: Compressing objects:  23% (5/21)   \u001b[K\rremote: Compressing objects:  28% (6/21)   \u001b[K\rremote: Compressing objects:  33% (7/21)   \u001b[K\rremote: Compressing objects:  38% (8/21)   \u001b[K\rremote: Compressing objects:  42% (9/21)   \u001b[K\rremote: Compressing objects:  47% (10/21)   \u001b[K\rremote: Compressing objects:  52% (11/21)   \u001b[K\rremote: Compressing objects:  57% (12/21)   \u001b[K\rremote: Compressing objects:  61% (13/21)   \u001b[K\rremote: Compressing objects:  66% (14/21)   \u001b[K\rremote: Compressing objects:  71% (15/21)   \u001b[K\rremote: Compressing objects:  76% (16/21)   \u001b[K\rremote: Compressing objects:  80% (17/21)   \u001b[K\rremote: Compressing objects:  85% (18/21)   \u001b[K\rremote: Compressing objects:  90% (19/21)   \u001b[K\rremote: Compressing objects:  95% (20/21)   \u001b[K\rremote: Compressing objects: 100% (21/21)   \u001b[K\rremote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "Unpacking objects:   3% (1/27)   \rUnpacking objects:   7% (2/27)   \rUnpacking objects:  11% (3/27)   \rUnpacking objects:  14% (4/27)   \rUnpacking objects:  18% (5/27)   \rUnpacking objects:  22% (6/27)   \rUnpacking objects:  25% (7/27)   \rUnpacking objects:  29% (8/27)   \rUnpacking objects:  33% (9/27)   \rUnpacking objects:  37% (10/27)   \rUnpacking objects:  40% (11/27)   \rUnpacking objects:  44% (12/27)   \rUnpacking objects:  48% (13/27)   \rUnpacking objects:  51% (14/27)   \rUnpacking objects:  55% (15/27)   \rremote: Total 27 (delta 6), reused 24 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (27/27), done.\n",
            "Fetching checkpoint: 1.00kit [00:00, 1.01Mit/s]                                                     \n",
            "Fetching encoder.json: 1.04Mit [00:00, 52.3Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.06Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:07, 67.9Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 5.47Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 50.0Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 48.5Mit/s]                                                       \n",
            "Fetching checkpoint: 1.00kit [00:00, 924kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 51.2Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 836kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:26, 52.7Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 7.93Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 45.6Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 41.5Mit/s]                                                       \n",
            "Requirement already satisfied: fire>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.1.3)\n",
            "Requirement already satisfied: regex==2018.1.10 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (2018.1.10)\n",
            "Requirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Requirement already satisfied: tqdm==4.31.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.31.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjrdHbh8tinV",
        "colab_type": "text"
      },
      "source": [
        "### 2. Test with the interactive model\n",
        "\n",
        "There are a few flags available, with a default value: \n",
        "- `seed = None`  || a random value is generated unless specified. give a specific integer value if you want to reproduce same results in the future.\n",
        "- `nsamples = 1`     ||  specify the number of samples you want to print\n",
        "- `length = None`   ||  number of tokens (words) to print on each sample.\n",
        "- `batch_size= 1`  ||  how many inputs you want to process simultaneously. *doesn't seem to affect the results. * \n",
        "- `temperature = 1`  ||  scales logits before sampling prior to softmax.\n",
        "- `top_k = 0`   ||  truncates the set of logits considered to those with the highest values.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56cSU05StiuQ",
        "colab_type": "code",
        "outputId": "5036e532-e2de-4493-ca0e-fd00828f0777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py -- --help"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type:        function\n",
            "String form: <function interact_model at 0x7fca17d17d08>\n",
            "File:        /content/gpt-2-twitter-bot/gpt-2-twitter-bot/src/interactive_conditional_samples.py\n",
            "Line:        11\n",
            "Docstring:   Interactively run the model\n",
            ":model_name=117M : String, which model to use\n",
            ":seed=None : Integer seed for random number generators, fix seed to reproduce\n",
            " results\n",
            ":nsamples=1 : Number of samples to return total\n",
            ":batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
            ":length=None : Number of tokens in generated text, if None (default), is\n",
            " determined by model hyperparameters\n",
            ":temperature=1 : Float value controlling randomness in boltzmann\n",
            " distribution. Lower temperature results in less random completions. As the\n",
            " temperature approaches zero, the model will become deterministic and\n",
            " repetitive. Higher temperature results in more random completions.\n",
            ":top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
            " considered for each step (token), resulting in deterministic completions,\n",
            " while 40 means 40 words are considered at each step. 0 (default) is a\n",
            " special setting meaning no restrictions. 40 generally is a good value.\n",
            " :models_dir : path to parent folder containing model subfolders\n",
            " (i.e. contains the <model_name> folder)     \n",
            "\n",
            "Usage:       interactive_conditional_samples.py [MODEL_NAME] [SEED] [NSAMPLES] [BATCH_SIZE] [LENGTH] [TEMPERATURE] [TOP_K] [MODELS_DIR]\n",
            "             interactive_conditional_samples.py [--model-name MODEL_NAME] [--seed SEED] [--nsamples NSAMPLES] [--batch-size BATCH_SIZE] [--length LENGTH] [--temperature TEMPERATURE] [--top-k TOP_K] [--models-dir MODELS_DIR]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjaG_szKvYRY",
        "colab_type": "text"
      },
      "source": [
        "### 3. Load Twitter Credentials\n",
        "  [Twitter Account link](https://twitter.com/ai_telling) <br>\n",
        "[Twitter Developer link](https://developer.twitter.com/en/apps/16374336)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy_CVHFevXer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Enter your twitter account detail here\n",
        "consumer_key = ''\n",
        "consumer_secret = ''\n",
        "access_token = ''\n",
        "access_token_secret = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP_friHbti4p",
        "colab_type": "text"
      },
      "source": [
        "### 3. Twitter API connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVJCsE3UvM9m",
        "colab_type": "code",
        "outputId": "d9920909-52de-494b-8303-47d4c59ace3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tweepy\n",
        "from time import sleep\n",
        "\n",
        "# Code to access the account\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "user = api.me()\n",
        "print(\"This is my user name: \"+user.name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is my user name: AI Story Telling\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8j19F9dps4d",
        "colab_type": "text"
      },
      "source": [
        "### Using GPT-2 model to create tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-p1JFiaps74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy all the files from src to pwd\n",
        "!cp -a ./src/. ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWd-Fejjf4_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the interact model\n",
        "from interactive_conditional_colab import interact_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYcQvxMB2KkK",
        "colab_type": "text"
      },
      "source": [
        "### Provide input text\n",
        "To be automated - based on trendy topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-svxDHb7YqJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up input\n",
        "input_text = \"Once upon a time,\"\n",
        "end_text = '#TweetCreatedByAI'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUryByin2Lbg",
        "colab_type": "code",
        "outputId": "e497d98b-df6b-40a3-ea97-2d1d134e0d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# run the model\n",
        "interact_model(input_text=input_text,\n",
        "               temperature=0.8)\n",
        "\n",
        "# cleaning the ouput\n",
        "f=open(\"GPT-2_output.txt\", \"r\")\n",
        "context = f.read()\n",
        "\n",
        "tweet_raw = context.split(\"=========Start==========\")[0]\n",
        "tweet_split = re.split('! |\\.',tweet_raw)\n",
        "tweet_split_len = [len(scentence)+1 for scentence in tweet_split]\n",
        "tweet_split_len_cum = np.cumsum(tweet_split_len)\n",
        "\n",
        "# max length limit is 280 characters\n",
        "tweet_len = 0 # initialise \n",
        "tweet_len_limit = 280 - len(end_text) - 4\n",
        "for length in tweet_split_len_cum:\n",
        "  if length <= tweet_len_limit:\n",
        "    tweet_len = length\n",
        "  else:\n",
        "    print(\"Maximum tweet length: \" + str(tweet_len))\n",
        "    break\n",
        "\n",
        "gpt2_input = tweet_raw[:tweet_split_len_cum[0]] + \"\\n\"\n",
        "gpt2_output=tweet_raw[tweet_split_len_cum[0]+1:tweet_len] + \"\\n\"\n",
        "\n",
        "tweet_final = gpt2_input + gpt2_output + end_text\n",
        "print(\"=\"*30)\n",
        "print(tweet_final)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2-twitter-bot/gpt-2-twitter-bot/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2-twitter-bot/gpt-2-twitter-bot/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from models/345M/model.ckpt\n",
            "Once upon a time,\n",
            "======================================== SAMPLE 1 ========================================\n",
            " the energy at the heart of modern democracy was embodied in the young boys who had assembled at the nation's capital to hear FDR deliver the Great Society speech in 1981. The Age of Reagan, for instance, should have included an account of the Clinton administration's role in running down the economy and privatizing Social Security. There was simply no reason to include this so-called \"tough on crime\" agenda in the Clinton administration's history, which Bill Clinton later tried to paint as a paragon of anti-crime initiatives, and which ended up leading to the deaths of countless innocent people.\n",
            "\n",
            "But we have come a long way in as a nation in the past few decades, and so the policy prescriptions of old timeers like FDR are probably no longer necessary. There is only one major reason for this: America has lost control of the global economy, and this is a loss that is becoming ever more real.\n",
            "\n",
            "The Global Financial Crisis, Was It Fraud?\n",
            "\n",
            "The financial crisis met its focus when a large amount of financial transactions that were carried out through banks and hedge funds were not recognized as income. As a result, the U.S. government was left with a budget deficit, which real estate banks and investment funds capitalized on by securitizing the funds in order to borrow money at substantial interest rates. The Fed's monetary policy fired the brakes on this process, but it did nothing to fix the underlying problem.\n",
            "\n",
            "The result, as noted by commentator George L. Friedman, was \"a global financial bubble which has burst.\" One of the major reasons for this crisis was the increase in the size of the world economy during the past few decades. In 1989, the world economy was around 700 trillion dollars. By 2004, it had expanded by 700 trillion dollars. The rapid increase in economic activity led to a corresponding increase in the size of the global supply of money, allowing for an enormous increase in the demand to buy, use, and finance goods and services. This increased demand led to the accumulation of gigantic amounts of derivatives, pushing up prices and making it harder and harder for investment banks and hedge funds to get the needed assets refiled at prices below what they were producing.\n",
            "\n",
            "These concerns about \"too big to fail\" banks also led to widespread disruption of payments systems in the financial system. By raising interest rates and cutting transaction costs, these financial institutions made it possible for them to start pre-selling traditional goods and services to buyers and sellers at a higher rate than the amount they were actually producing.\n",
            "Maximum tweet length: 188\n",
            "==============================\n",
            "Once upon a time, the energy at the heart of modern democracy was embodied in the young boys who had assembled at the nation's capital to hear FDR deliver the Great Society speech in 1981.\n",
            "\n",
            "#TweetCreatedByAI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGsNWDzj2z25",
        "colab_type": "code",
        "outputId": "78fc432a-2927-4b57-efed-752f4b47bbaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Post the above tweet online\n",
        "api.update_status(tweet_final)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Status(_api=<tweepy.api.API object at 0x7f32fa96ccc0>, _json={'created_at': 'Sun Jun 09 08:48:04 +0000 2019', 'id': 1137642505737056259, 'id_str': '1137642505737056259', 'text': 'Once upon a time, the energy at the heart of modern democracy was embodied in the young boys who had assembled at tâ€¦ https://t.co/G5M1Or5ZyI', 'truncated': True, 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [], 'urls': [{'url': 'https://t.co/G5M1Or5ZyI', 'expanded_url': 'https://twitter.com/i/web/status/1137642505737056259', 'display_url': 'twitter.com/i/web/status/1â€¦', 'indices': [117, 140]}]}, 'source': '<a href=\"https://github.com/openai/gpt-2\" rel=\"nofollow\">AI Story Telling</a>', 'in_reply_to_status_id': None, 'in_reply_to_status_id_str': None, 'in_reply_to_user_id': None, 'in_reply_to_user_id_str': None, 'in_reply_to_screen_name': None, 'user': {'id': 1128859106607976448, 'id_str': '1128859106607976448', 'name': 'AI Story Telling', 'screen_name': 'ai_telling', 'location': 'Sydney, New South Wales', 'description': 'AI generated Tweets by the GPT-2 model. Aspired to create meaningful tweets for users.ðŸ˜€', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 1, 'friends_count': 0, 'listed_count': 0, 'created_at': 'Thu May 16 03:05:59 +0000 2019', 'favourites_count': 0, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 22, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': '000000', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1130108335665995776/GDOeGc4R_normal.png', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1130108335665995776/GDOeGc4R_normal.png', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/1128859106607976448/1558274059', 'profile_link_color': '1B95E0', 'profile_sidebar_border_color': '000000', 'profile_sidebar_fill_color': '000000', 'profile_text_color': '000000', 'profile_use_background_image': False, 'has_extended_profile': True, 'default_profile': False, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}, 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'retweet_count': 0, 'favorite_count': 0, 'favorited': False, 'retweeted': False, 'lang': 'en'}, created_at=datetime.datetime(2019, 6, 9, 8, 48, 4), id=1137642505737056259, id_str='1137642505737056259', text='Once upon a time, the energy at the heart of modern democracy was embodied in the young boys who had assembled at tâ€¦ https://t.co/G5M1Or5ZyI', truncated=True, entities={'hashtags': [], 'symbols': [], 'user_mentions': [], 'urls': [{'url': 'https://t.co/G5M1Or5ZyI', 'expanded_url': 'https://twitter.com/i/web/status/1137642505737056259', 'display_url': 'twitter.com/i/web/status/1â€¦', 'indices': [117, 140]}]}, source='AI Story Telling', source_url='https://github.com/openai/gpt-2', in_reply_to_status_id=None, in_reply_to_status_id_str=None, in_reply_to_user_id=None, in_reply_to_user_id_str=None, in_reply_to_screen_name=None, author=User(_api=<tweepy.api.API object at 0x7f32fa96ccc0>, _json={'id': 1128859106607976448, 'id_str': '1128859106607976448', 'name': 'AI Story Telling', 'screen_name': 'ai_telling', 'location': 'Sydney, New South Wales', 'description': 'AI generated Tweets by the GPT-2 model. Aspired to create meaningful tweets for users.ðŸ˜€', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 1, 'friends_count': 0, 'listed_count': 0, 'created_at': 'Thu May 16 03:05:59 +0000 2019', 'favourites_count': 0, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 22, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': '000000', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1130108335665995776/GDOeGc4R_normal.png', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1130108335665995776/GDOeGc4R_normal.png', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/1128859106607976448/1558274059', 'profile_link_color': '1B95E0', 'profile_sidebar_border_color': '000000', 'profile_sidebar_fill_color': '000000', 'profile_text_color': '000000', 'profile_use_background_image': False, 'has_extended_profile': True, 'default_profile': False, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}, id=1128859106607976448, id_str='1128859106607976448', name='AI Story Telling', screen_name='ai_telling', location='Sydney, New South Wales', description='AI generated Tweets by the GPT-2 model. Aspired to create meaningful tweets for users.ðŸ˜€', url=None, entities={'description': {'urls': []}}, protected=False, followers_count=1, friends_count=0, listed_count=0, created_at=datetime.datetime(2019, 5, 16, 3, 5, 59), favourites_count=0, utc_offset=None, time_zone=None, geo_enabled=False, verified=False, statuses_count=22, lang=None, contributors_enabled=False, is_translator=False, is_translation_enabled=False, profile_background_color='000000', profile_background_image_url='http://abs.twimg.com/images/themes/theme1/bg.png', profile_background_image_url_https='https://abs.twimg.com/images/themes/theme1/bg.png', profile_background_tile=False, profile_image_url='http://pbs.twimg.com/profile_images/1130108335665995776/GDOeGc4R_normal.png', profile_image_url_https='https://pbs.twimg.com/profile_images/1130108335665995776/GDOeGc4R_normal.png', profile_banner_url='https://pbs.twimg.com/profile_banners/1128859106607976448/1558274059', profile_link_color='1B95E0', profile_sidebar_border_color='000000', profile_sidebar_fill_color='000000', profile_text_color='000000', profile_use_background_image=False, has_extended_profile=True, default_profile=False, default_profile_image=False, following=False, follow_request_sent=False, notifications=False, translator_type='none'), user=User(_api=<tweepy.api.API object at 0x7f32fa96ccc0>, _json={'id': 1128859106607976448, 'id_str': '1128859106607976448', 'name': 'AI Story Telling', 'screen_name': 'ai_telling', 'location': 'Sydney, New South Wales', 'description': 'AI generated Tweets by the GPT-2 model. Aspired to create meaningful tweets for users.ðŸ˜€', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 1, 'friends_count': 0, 'listed_count': 0, 'created_at': 'Thu May 16 03:05:59 +0000 2019', 'favourites_count': 0, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 22, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': '000000', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1130108335665995776/GDOeGc4R_normal.png', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1130108335665995776/GDOeGc4R_normal.png', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/1128859106607976448/1558274059', 'profile_link_color': '1B95E0', 'profile_sidebar_border_color': '000000', 'profile_sidebar_fill_color': '000000', 'profile_text_color': '000000', 'profile_use_background_image': False, 'has_extended_profile': True, 'default_profile': False, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}, id=1128859106607976448, id_str='1128859106607976448', name='AI Story Telling', screen_name='ai_telling', location='Sydney, New South Wales', description='AI generated Tweets by the GPT-2 model. Aspired to create meaningful tweets for users.ðŸ˜€', url=None, entities={'description': {'urls': []}}, protected=False, followers_count=1, friends_count=0, listed_count=0, created_at=datetime.datetime(2019, 5, 16, 3, 5, 59), favourites_count=0, utc_offset=None, time_zone=None, geo_enabled=False, verified=False, statuses_count=22, lang=None, contributors_enabled=False, is_translator=False, is_translation_enabled=False, profile_background_color='000000', profile_background_image_url='http://abs.twimg.com/images/themes/theme1/bg.png', profile_background_image_url_https='https://abs.twimg.com/images/themes/theme1/bg.png', profile_background_tile=False, profile_image_url='http://pbs.twimg.com/profile_images/1130108335665995776/GDOeGc4R_normal.png', profile_image_url_https='https://pbs.twimg.com/profile_images/1130108335665995776/GDOeGc4R_normal.png', profile_banner_url='https://pbs.twimg.com/profile_banners/1128859106607976448/1558274059', profile_link_color='1B95E0', profile_sidebar_border_color='000000', profile_sidebar_fill_color='000000', profile_text_color='000000', profile_use_background_image=False, has_extended_profile=True, default_profile=False, default_profile_image=False, following=False, follow_request_sent=False, notifications=False, translator_type='none'), geo=None, coordinates=None, place=None, contributors=None, is_quote_status=False, retweet_count=0, favorite_count=0, favorited=False, retweeted=False, lang='en')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdwAXFicwMI1",
        "colab_type": "text"
      },
      "source": [
        "--------------\n",
        "# END OF THE SCRIPT"
      ]
    }
  ]
}